{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tobiasSviderski/assigmentDataScienceProduct/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true,
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCx_zoCkUyC6",
        "outputId": "12e7c176-d7db-4513-b7a2-a1c200868813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# CETM47 Assigment 2 \n",
        "from sklearn.datasets import fetch_20newsgroups # Download the dataset\n",
        "\n",
        "# Word stop and lemming packages\n",
        "import nltk\n",
        "import string \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Pipeline packages\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "\n",
        "# Models\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Preprocessing for MultinomialNB model with GensimWord2VecVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss\n",
        "\n",
        "# Needed files for it to work\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "\n",
        "class GensimWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Word vectors are averaged across to create the document-level vectors/features.\n",
        "\n",
        "    gensim's own gensim.sklearn_api.W2VTransformer doesn't support out of vocabulary words,\n",
        "    hence we roll out our own.\n",
        "\n",
        "    All the parameters are gensim.models.Word2Vec's parameters.\n",
        "\n",
        "    https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vector_size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None,\n",
        "                sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5,\n",
        "                ns_exponent=0.75, cbow_mean=1, hashfxn=hash, epochs=5, null_word=0,\n",
        "                trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False,\n",
        "                callbacks=(), max_final_vocab=None, stopwords_action=None):\n",
        "\n",
        "        self.vector_size = vector_size\n",
        "        self.alpha = alpha\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.sample = sample\n",
        "        self.seed = seed\n",
        "        self.workers = workers\n",
        "        self.min_alpha = min_alpha\n",
        "        self.sg = sg\n",
        "        self.hs = hs\n",
        "        self.negative = negative\n",
        "        self.ns_exponent = ns_exponent\n",
        "        self.cbow_mean = cbow_mean\n",
        "        self.hashfxn = hashfxn\n",
        "        self.epochs = epochs\n",
        "        self.null_word = null_word\n",
        "        self.trim_rule = trim_rule\n",
        "        self.sorted_vocab = sorted_vocab\n",
        "        self.batch_words = batch_words\n",
        "        self.compute_loss = compute_loss\n",
        "        self.callbacks = callbacks\n",
        "        self.max_final_vocab = max_final_vocab\n",
        "        self.stopwords_action = stopwords_action\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.model_ = Word2Vec(\n",
        "            sentences=X, corpus_file=None,\n",
        "            vector_size=self.vector_size, alpha=self.alpha, window=self.window, min_count=self.min_count,\n",
        "            max_vocab_size=self.max_vocab_size, sample=self.sample, seed=self.seed,\n",
        "            workers=self.workers, min_alpha=self.min_alpha, sg=self.sg, hs=self.hs,\n",
        "            negative=self.negative, ns_exponent=self.ns_exponent, cbow_mean=self.cbow_mean,\n",
        "            hashfxn=self.hashfxn, epochs=self.epochs, null_word=self.null_word,\n",
        "            trim_rule=self.trim_rule, sorted_vocab=self.sorted_vocab, batch_words=self.batch_words,\n",
        "            compute_loss=self.compute_loss, callbacks=self.callbacks,\n",
        "            max_final_vocab=self.max_final_vocab)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_embeddings = np.array([self._get_embedding(words) for words in X])\n",
        "        return X_embeddings\n",
        "\n",
        "    def _get_embedding(self, words):\n",
        "        lower_words = words.lower() # Return lowercase string\n",
        "        no_special_words = re.sub('[^.,\\-\\'!?a-zA-Z0-9 \\n\\.]', '', lower_words) # Remove special characters\n",
        "        # If stopwords are true\n",
        "        if self.stopwords_action is not None:\n",
        "          no_stopwords_words = [word for word in no_special_words if not word in self.stopwords_action]\n",
        "          no_special_words = (\"\").join(no_stopwords_words)\n",
        "\n",
        "        print(\"no special words\", no_special_words)\n",
        "        valid_words = [word for word in no_special_words if word in self.model_.wv.key_to_index]\n",
        "        print(valid_words)\n",
        "\n",
        "        import pdb\n",
        "        pdb.set_trace()\n",
        "        if valid_words:\n",
        "            embedding = np.zeros((len(valid_words), self.vector_size), dtype=np.float32)\n",
        "            for idx, word in enumerate(valid_words):\n",
        "                embedding[idx] = self.model_.wv[word]\n",
        "\n",
        "            return np.mean(embedding, axis=0)\n",
        "        else:\n",
        "            return np.zeros(self.vector_size)"
      ],
      "metadata": {
        "id": "QOvvT4SAYCl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zSkPAHcXUyDB"
      },
      "outputs": [],
      "source": [
        "#Load the datasets\n",
        "#Loading the train set\n",
        "newsgroup_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "#Loading the test set\n",
        "newsgroup_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "aUXrPIfVUyDC"
      },
      "outputs": [],
      "source": [
        "#Overview of the steps\n",
        "#Running basic pipeline without anything\n",
        "#Running pipeline with noise removal and lower casing\n",
        "#Running pipeline with lemming and stop words\n",
        "#Running pipeline with word2vec embedding\n",
        "\n",
        "# Options that the model would be trying\n",
        "use_noise_lower = \"noise_lower\"\n",
        "use_lemming = \"lemming\"\n",
        "use_stopwords = \"stopwords\"\n",
        "\n",
        "# Get every combination of the options\n",
        "import itertools\n",
        "combinations = [use_noise_lower, use_lemming, use_stopwords]\n",
        "every_combination = [list(zip(combinations, x)) for x in itertools.product([True, False], repeat=len(combinations))] #Generate all combinations\n",
        "\n",
        "# Classifiers to try\n",
        "classifiers = [\n",
        "    MultinomialNB(alpha=0.1),\n",
        "    # DecisionTreeClassifier(),\n",
        "    # RandomForestClassifier(),\n",
        "    # Add the MLP one \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9r46G5pTUyDD"
      },
      "outputs": [],
      "source": [
        "def runModel(classifier):\n",
        "    print(classifier)\n",
        "    for options in every_combination:\n",
        "        lemming_value = next(x for x in options if x[0] == use_lemming)[1]\n",
        "        noise_lower_value = next(x for x in options if x[0] == use_noise_lower)[1]\n",
        "        stopwords_value = next(x for x in options if x[0] == use_stopwords)[1]\n",
        "        print(options)\n",
        "        runPipelineNormal(classifier, lemming_value, noise_lower_value, stopwords_value)\n",
        "\n",
        "    #Run pipeline with word2vec\n",
        "    print(\"Running pipeline with word2vec\")\n",
        "    runPipelineWordEmbeddings(classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5ydKAmq7UyDE"
      },
      "outputs": [],
      "source": [
        "def runPipelineNormal(classifier, lemming_value, noise_lower_value, stopwords_value):\n",
        "    # Create the pipeline\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('clf', classifier),\n",
        "    ])\n",
        "\n",
        "    ## Lemming logic\n",
        "    def tokenize(text):\n",
        "        # Remove punctuation\n",
        "        text = ''.join([ch for ch in text if ch not in string.punctuation])\n",
        "        tokens = word_tokenize(text) # Get tokenize text\n",
        "\n",
        "        if stopwords_value & lemming_value: \n",
        "        # Remove stopwords\n",
        "            tokens = [w for w in tokens if w not in stopwords.words('english')]\n",
        "\n",
        "        # If lemming is true, lemmatize the words\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    if lemming_value: \n",
        "        pipeline.set_params(vect__tokenizer=tokenize)\n",
        "\n",
        "    # If stopwords is true and lemming is false, remove stopwords\n",
        "    if stopwords_value and not lemming_value:\n",
        "        pipeline.set_params(vect__stop_words=stopwords.words('english'))\n",
        "\n",
        "    # If noise_lower is true, remove noise\n",
        "    if noise_lower_value:\n",
        "        pipeline.set_params(vect__ngram_range=(1, 2))\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    pipeline.fit(newsgroup_train.data, newsgroup_train.target)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    ## Get the accuracy\n",
        "    predicted_accuracy = accuracy_score(newsgroup_test.target, pipeline.predict(newsgroup_test.data))\n",
        "    ## Get the log loss\n",
        "    predicted_log_loss = log_loss(newsgroup_test.target, pipeline.predict_proba(newsgroup_test.data))\n",
        "\n",
        "    print(\"Accuracy: {}\".format(predicted_accuracy))\n",
        "    print(\"Log Loss: {}\".format(predicted_log_loss))\n",
        "    ## Append the results\n",
        "    return{\n",
        "        'accuracy': predicted_accuracy,\n",
        "        'log_loss': predicted_log_loss,\n",
        "        # 'confusion_matrix': predicted_confusion_matrix,\n",
        "        # 'classification_report': predicted_classification_report\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NAwLKzaHUyDF"
      },
      "outputs": [],
      "source": [
        "def runPipelineWordEmbeddings(classifier):\n",
        "    pipeline = Pipeline([\n",
        "        ('w2v', GensimWord2VecVectorizer(vector_size=50, min_count=3, sg=1, \n",
        "                                         alpha=0.025)),\n",
        "        ('scale', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
        "        ('clf', classifier),\n",
        "    ])\n",
        "\n",
        "      # if stopwords_value and not lemming_value:\n",
        "    pipeline.set_params(w2v__stopwords_action=stopwords.words('english'))\n",
        "\n",
        "    # Train the model\n",
        "    pipeline.fit(newsgroup_train.data, newsgroup_train.target)\n",
        "\n",
        "        # Train the model\n",
        "    pipeline.fit(newsgroup_train.data, newsgroup_train.target)\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    ## Get the accuracy\n",
        "    predicted_accuracy = accuracy_score(newsgroup_test.target, pipeline.predict(newsgroup_test.data))\n",
        "    ## Get the log loss\n",
        "    predicted_log_loss = log_loss(newsgroup_test.target, pipeline.predict_proba(newsgroup_test.data))\n",
        "\n",
        "    print(\"Accuracy: {}\".format(predicted_accuracy))\n",
        "    print(\"Log Loss: {}\".format(predicted_log_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "CJWGYISRUyDG",
        "outputId": "2f1118ef-9a42-413e-d470-1ebc47ea1830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "no special words  w wnerng f nne u here cul enlghen e n h cr  w\n",
            "he her .  w  2-r pr cr, lke  be fr he le 60\n",
            "erl 70.  w clle  brckln. he r were rell ll. n n,\n",
            "he frn buper w epre fr he re f he b. h  \n",
            "ll  knw. f nne cn elle  el ne, engne pec, er\n",
            "f prucn, where h cr  e, hr, r whever nf u\n",
            "hve n h funk lkng cr, plee e-l.\n",
            "[' ', 'w', ' ', 'w', 'n', 'e', 'r', 'n', 'g', ' ', 'f', ' ', 'n', 'n', 'e', ' ', 'u', ' ', 'h', 'e', 'r', 'e', ' ', 'c', 'u', 'l', ' ', 'e', 'n', 'l', 'g', 'h', 'e', 'n', ' ', 'e', ' ', 'n', ' ', 'h', ' ', 'c', 'r', ' ', ' ', 'w', '\\n', 'h', 'e', ' ', 'h', 'e', 'r', ' ', '.', ' ', ' ', 'w', ' ', ' ', '2', '-', 'r', ' ', 'p', 'r', ' ', 'c', 'r', ',', ' ', 'l', 'k', 'e', ' ', ' ', 'b', 'e', ' ', 'f', 'r', ' ', 'h', 'e', ' ', 'l', 'e', ' ', '6', '0', '\\n', 'e', 'r', 'l', ' ', '7', '0', '.', ' ', ' ', 'w', ' ', 'c', 'l', 'l', 'e', ' ', ' ', 'b', 'r', 'c', 'k', 'l', 'n', '.', ' ', 'h', 'e', ' ', 'r', ' ', 'w', 'e', 'r', 'e', ' ', 'r', 'e', 'l', 'l', ' ', 'l', 'l', '.', ' ', 'n', ' ', 'n', ',', '\\n', 'h', 'e', ' ', 'f', 'r', 'n', ' ', 'b', 'u', 'p', 'e', 'r', ' ', 'w', ' ', 'e', 'p', 'r', 'e', ' ', 'f', 'r', ' ', 'h', 'e', ' ', 'r', 'e', ' ', 'f', ' ', 'h', 'e', ' ', 'b', '.', ' ', 'h', ' ', ' ', '\\n', 'l', 'l', ' ', ' ', 'k', 'n', 'w', '.', ' ', 'f', ' ', 'n', 'n', 'e', ' ', 'c', 'n', ' ', 'e', 'l', 'l', 'e', ' ', ' ', 'e', 'l', ' ', 'n', 'e', ',', ' ', 'e', 'n', 'g', 'n', 'e', ' ', 'p', 'e', 'c', ',', ' ', 'e', 'r', '\\n', 'f', ' ', 'p', 'r', 'u', 'c', 'n', ',', ' ', 'w', 'h', 'e', 'r', 'e', ' ', 'h', ' ', 'c', 'r', ' ', ' ', 'e', ',', ' ', 'h', 'r', ',', ' ', 'r', ' ', 'w', 'h', 'e', 'v', 'e', 'r', ' ', 'n', 'f', ' ', 'u', '\\n', 'h', 'v', 'e', ' ', 'n', ' ', 'h', ' ', 'f', 'u', 'n', 'k', ' ', 'l', 'k', 'n', 'g', ' ', 'c', 'r', ',', ' ', 'p', 'l', 'e', 'e', ' ', 'e', '-', 'l', '.']\n",
            "> <ipython-input-50-fc14f9f909c8>(81)_get_embedding()\n",
            "-> if valid_words:\n",
            "--KeyboardInterrupt--\n",
            "--KeyboardInterrupt--\n",
            "(Pdb) exit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BdbQuit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-c6e5592ae2ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     runModel(classifier)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrunPipelineWordEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-35-d22ee43f13a7>\u001b[0m in \u001b[0;36mrunPipelineWordEmbeddings\u001b[0;34m(classifier)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsgroup_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewsgroup_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[1;32m    389\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m             )\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-fc14f9f909c8>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mX_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-fc14f9f909c8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mX_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-fc14f9f909c8>\u001b[0m in \u001b[0;36m_get_embedding\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mvalid_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-fc14f9f909c8>\u001b[0m in \u001b[0;36m_get_embedding\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mvalid_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBdbQuit\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# for classifier in classifiers:\n",
        "#     runModel(classifier)\n",
        "\n",
        "runPipelineWordEmbeddings(classifiers[0])"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "0f065bf5083101ff4183af16d6dd7e1e8693fcca6c8d3092415739d8257b5fcf"
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "5ccc0d954e93423bad5f61678c795cb85e1b45280f8b7aa41676f763ccf4035c"
      }
    },
    "colab": {
      "name": "pipeline.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}